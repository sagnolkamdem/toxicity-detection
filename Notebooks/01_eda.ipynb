{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploratory Datat Analysis",
   "id": "b0fb93ef94b770fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T14:10:26.051034Z",
     "start_time": "2025-12-29T14:10:25.248330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('../Data/train.csv')\n",
    "data.head()\n"
   ],
   "id": "e9212eab77350437",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "24f43b339ef80755"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T14:10:32.407040Z",
     "start_time": "2025-12-29T14:10:32.393856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"total samples: {data.shape[0]}\")\n",
    "print(\"*\"*50)\n",
    "print(f\"total features: {data.shape[1]}\")"
   ],
   "id": "896c291b4698cddf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples: 159571\n",
      "**************************************************\n",
      "total features: 8\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T14:10:34.846294Z",
     "start_time": "2025-12-29T14:10:34.687067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(data.info())\n",
    "print(data.describe())"
   ],
   "id": "5f09fd1e9ccf55c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n",
      "None\n",
      "               toxic   severe_toxic        obscene         threat  \\\n",
      "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
      "mean        0.095844       0.009996       0.052948       0.002996   \n",
      "std         0.294379       0.099477       0.223931       0.054650   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "              insult  identity_hate  \n",
      "count  159571.000000  159571.000000  \n",
      "mean        0.049364       0.008805  \n",
      "std         0.216627       0.093420  \n",
      "min         0.000000       0.000000  \n",
      "25%         0.000000       0.000000  \n",
      "50%         0.000000       0.000000  \n",
      "75%         0.000000       0.000000  \n",
      "max         1.000000       1.000000  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T14:10:38.304941Z",
     "start_time": "2025-12-29T14:10:37.730959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 42\n",
    "LLM_SAMPLE_SIZE = 200\n",
    "\n",
    "# STEP 1: LOAD & CONSOLIDATE LABELS\n",
    "df = data\n",
    "\n",
    "toxicity_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Create a single binary target: If any column is 1, the text is Toxic (1)\n",
    "df['final_label'] = df[toxicity_cols].max(axis=1)\n",
    "\n",
    "print(f\"Data loaded. Total rows: {len(df)}\")\n",
    "print(f\"Toxic samples: {df['final_label'].sum()} | Non-Toxic samples: {len(df) - df['final_label'].sum()}\")\n",
    "\n",
    "\n",
    "# STEP 2: SPLIT & CREATE GOLDEN SAMPLE (80/20)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Now, extract the small \"Golden Sample\" specifically for the LLM\n",
    "# We use .copy() to avoid SettingWithCopy warnings\n",
    "llm_sample = test_df.sample(n=LLM_SAMPLE_SIZE, random_state=SEED).copy()\n",
    "\n",
    "print(f\"LLM Sample of {len(llm_sample)} rows (from Test set).\")\n",
    "\n",
    "# STEP 3: MINIMAL CLEANING\n",
    "llm_sample['cleaned_text'] = llm_sample['comment_text'].str.replace('\\n', ' ', regex=False)\n",
    "\n",
    "# STEP 4: PROMPT ENGINEERING\n",
    "prompt_template = \"\"\"You are the best content moderator in the history.\n",
    "                     If you fail in your job, your company will shutdown.\n",
    "                     Classify the following text as 'Toxic' or 'Non-Toxic'.\n",
    "                     Reply with ONLY one word.\n",
    "\n",
    "                     Text: \"{}\"\n",
    "                     Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Apply the template to every row\n",
    "llm_sample['llm_prompt'] = llm_sample['cleaned_text'].apply(lambda x: prompt_template.format(x))\n",
    "\n",
    "\n",
    "# STEP 5: SAVE ARTIFACTS\n",
    "output_cols = ['id', 'final_label', 'llm_prompt']\n",
    "llm_sample[output_cols].to_csv('../Data/llm/llm_baseline_inputs.csv', index=False)\n",
    "\n",
    "# OPTIONAL: Save the indices/IDs to a separate file so you can filter\n",
    "# the same rows when you evaluate RoBERTa later.\n",
    "llm_sample['id'].to_csv('../Data/RoBERTa/golden_sample_ids.csv', index=False)\n",
    "\n",
    "print(\"\\nProcessing complete!\")\n",
    "print(\"1. 'llm_baseline_inputs.csv' -> Feed this to your LLM.\")\n",
    "print(\"2. 'golden_sample_ids.csv' -> Use this to filter RoBERTa predictions later.\")"
   ],
   "id": "b0188239f82c011e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Total rows: 159571\n",
      "Toxic samples: 16225 | Non-Toxic samples: 143346\n",
      "LLM Sample of 200 rows (from Test set).\n",
      "\n",
      "Processing complete!\n",
      "1. 'llm_baseline_inputs.csv' -> Feed this to your LLM.\n",
      "2. 'golden_sample_ids.csv' -> Use this to filter RoBERTa predictions later.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3",
   "id": "aad7630416449ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T14:20:07.881713Z",
     "start_time": "2025-12-29T14:18:00.178676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# CONFIGURATION\n",
    "# Recommended: 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' for speed/testing\n",
    "# Recommended: 'mistralai/Mistral-7B-Instruct-v0.2' for better quality (requires GPU + quantization)\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "INPUT_FILE = '../Data/llm/llm_baseline_inputs.csv'\n",
    "OUTPUT_FILE = '../Data/llm/llm_baseline_results.csv'\n",
    "\n",
    "# 1. LOAD MODEL & TOKENIZER\n",
    "print(f\"Loading model: {MODEL_ID}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Load model with 4-bit quantization if using a large model on Colab (saves memory)\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    # quantization_config=quantization_config # Uncomment if using Mistral-7B\n",
    ")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=10,   # We only expect \"Toxic\" or \"Non-Toxic\"\n",
    "    temperature=0.1      # Low temp for deterministic results\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD DATA\n",
    "# ==========================================\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\"Loaded {len(df)} samples for inference.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. INFERENCE LOOP\n",
    "# ==========================================\n",
    "predictions = []\n",
    "latencies = []\n",
    "raw_outputs = []\n",
    "\n",
    "print(\"Starting inference...\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    prompt = row['llm_prompt']\n",
    "\n",
    "    # Measure Latency: Start Timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run Inference\n",
    "    # return_full_text=False ensures we only get the new generated part\n",
    "    result = pipe(prompt, return_full_text=False)[0]['generated_text']\n",
    "\n",
    "    # Measure Latency: Stop Timer\n",
    "    end_time = time.time()\n",
    "    latency_ms = (end_time - start_time) * 1000\n",
    "\n",
    "    # Store raw results\n",
    "    raw_outputs.append(result)\n",
    "    latencies.append(latency_ms)\n",
    "\n",
    "    # Simple Parsing Logic (Convert text to 0/1)\n",
    "    clean_result = result.strip().lower()\n",
    "    if \"non-toxic\" in clean_result:\n",
    "        pred = 0\n",
    "    elif \"toxic\" in clean_result:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 0 # Default fallback or handle as \"Unknown\" (-1)\n",
    "\n",
    "    predictions.append(pred)\n",
    "\n",
    "    if index % 10 == 0:\n",
    "        print(f\"Processed {index}/{len(df)} | Last Latency: {latency_ms:.2f}ms\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. SAVE RESULTS\n",
    "# ==========================================\n",
    "df['llm_raw_output'] = raw_outputs\n",
    "df['llm_pred_label'] = predictions\n",
    "df['latency_ms'] = latencies\n",
    "\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Done! Results saved to {OUTPUT_FILE}\")"
   ],
   "id": "304deed41a1cfb2b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagnolkamdem/projects/cours/5A/NLP & GenAI/Projet final/ NLP - final project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 samples for inference.\n",
      "Starting inference...\n",
      "Processed 0/200 | Last Latency: 37955.29ms\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 58\u001B[39m\n\u001B[32m     54\u001B[39m start_time = time.time()\n\u001B[32m     56\u001B[39m \u001B[38;5;66;03m# Run Inference\u001B[39;00m\n\u001B[32m     57\u001B[39m \u001B[38;5;66;03m# return_full_text=False ensures we only get the new generated part\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m58\u001B[39m result = \u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_full_text\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mgenerated_text\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     60\u001B[39m \u001B[38;5;66;03m# Measure Latency: Stop Timer\u001B[39;00m\n\u001B[32m     61\u001B[39m end_time = time.time()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cours/5A/NLP & GenAI/Projet final/ NLP - final project/.venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:332\u001B[39m, in \u001B[36mTextGenerationPipeline.__call__\u001B[39m\u001B[34m(self, text_inputs, **kwargs)\u001B[39m\n\u001B[32m    330\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    331\u001B[39m                 \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m(\u001B[38;5;28mlist\u001B[39m(chats), **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m332\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cours/5A/NLP & GenAI/Projet final/ NLP - final project/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1467\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1459\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[32m   1460\u001B[39m         \u001B[38;5;28miter\u001B[39m(\n\u001B[32m   1461\u001B[39m             \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   (...)\u001B[39m\u001B[32m   1464\u001B[39m         )\n\u001B[32m   1465\u001B[39m     )\n\u001B[32m   1466\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1467\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cours/5A/NLP & GenAI/Projet final/ NLP - final project/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1474\u001B[39m, in \u001B[36mPipeline.run_single\u001B[39m\u001B[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[39m\n\u001B[32m   1472\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[32m   1473\u001B[39m     model_inputs = \u001B[38;5;28mself\u001B[39m.preprocess(inputs, **preprocess_params)\n\u001B[32m-> \u001B[39m\u001B[32m1474\u001B[39m     model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1475\u001B[39m     outputs = \u001B[38;5;28mself\u001B[39m.postprocess(model_outputs, **postprocess_params)\n\u001B[32m   1476\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cours/5A/NLP & GenAI/Projet final/ NLP - final project/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1374\u001B[39m, in \u001B[36mPipeline.forward\u001B[39m\u001B[34m(self, model_inputs, **forward_params)\u001B[39m\n\u001B[32m   1372\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[32m   1373\u001B[39m         model_inputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_inputs, device=\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m-> \u001B[39m\u001B[32m1374\u001B[39m         model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1375\u001B[39m         model_outputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m   1376\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cours/5A/NLP & GenAI/Projet final/ NLP - final project/.venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:432\u001B[39m, in \u001B[36mTextGenerationPipeline._forward\u001B[39m\u001B[34m(self, model_inputs, **generate_kwargs)\u001B[39m\n\u001B[32m    429\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[32m    430\u001B[39m     generate_kwargs[\u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mself\u001B[39m.generation_config\n\u001B[32m--> \u001B[39m\u001B[32m432\u001B[39m output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    434\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, ModelOutput):\n\u001B[32m    435\u001B[39m     generated_sequence = output.sequences\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cours/5A/NLP & GenAI/Projet final/ NLP - final project/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cours/5A/NLP & GenAI/Projet final/ NLP - final project/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2564\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001B[39m\n\u001B[32m   2561\u001B[39m model_kwargs[\u001B[33m\"\u001B[39m\u001B[33muse_cache\u001B[39m\u001B[33m\"\u001B[39m] = generation_config.use_cache\n\u001B[32m   2563\u001B[39m \u001B[38;5;66;03m# 9. Call generation mode\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2564\u001B[39m result = \u001B[43mdecoding_method\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2565\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   2566\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2567\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2568\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2569\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2570\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgeneration_mode_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2571\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2572\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2574\u001B[39m \u001B[38;5;66;03m# Convert to legacy cache format if requested\u001B[39;00m\n\u001B[32m   2575\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2576\u001B[39m     generation_config.return_legacy_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   2577\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(result, \u001B[33m\"\u001B[39m\u001B[33mpast_key_values\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   2578\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(result.past_key_values, \u001B[33m\"\u001B[39m\u001B[33mto_legacy_cache\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   2579\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cours/5A/NLP & GenAI/Projet final/ NLP - final project/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2829\u001B[39m, in \u001B[36mGenerationMixin._sample\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[39m\n\u001B[32m   2827\u001B[39m     probs = nn.functional.softmax(next_token_scores, dim=-\u001B[32m1\u001B[39m)\n\u001B[32m   2828\u001B[39m     \u001B[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2829\u001B[39m     next_tokens = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmultinomial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m.squeeze(\u001B[32m1\u001B[39m)\n\u001B[32m   2830\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2831\u001B[39m     next_tokens = torch.argmax(next_token_scores, dim=-\u001B[32m1\u001B[39m)\n",
      "\u001B[31mRuntimeError\u001B[39m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
