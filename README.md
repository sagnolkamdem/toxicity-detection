# üõ°Ô∏è RoBERTa Toxicity Detector

A high-performance, low-latency AI system designed to detect toxic comments in online discussions. This project finetunes a **RoBERTa** model to achieve **~97% accuracy** while being **10x-50x faster** than Large Language Models (LLMs) like Mistral 7B.

## üìå Project Overview
The goal of this project is to build a production-ready toxicity detection service. It compares two approaches:
1.  **Zero-Shot LLM:** Using Mistral 7B (via API) to classify comments.
2.  **Finetuned Encoder:** Training `roberta-base` specifically for this task.
3.  **Developers:** Hind KHAYATI, Sagnol Boutal KAMDEM DJOKO, Pape Mamadou DIAGNE, Sarra HERELLI

**Key Findings:**
* **RoBERTa** achieved comparable accuracy to the LLM.
* **Latency:** RoBERTa runs in **~15-20ms** (GPU) / **~200ms** (CPU), compared to **2-5 seconds** for the LLM.
* **Cost:** Significantly cheaper to deploy and maintain.

---

## üìÇ Project Structure

```text
‚îú‚îÄ‚îÄ api/                            # Research & Training Notebooks
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                      # Research & Training Notebooks
‚îÇ   ‚îú‚îÄ‚îÄ 00_main_notebook.ipynb      # Recap of all steps
‚îÇ   ‚îú‚îÄ‚îÄ 01_eda.ipynb                # Exploratory & Data Analysis
‚îÇ   ‚îú‚îÄ‚îÄ 02_llm.ipynb                # LLM baseline
‚îÇ   ‚îú‚îÄ‚îÄ 03_RoBERTa_training.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 04_Latency_Comparison.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 05_Explainability_SHAP.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 06_fairness_bias.ipynb
‚îÇ
‚îú‚îÄ‚îÄ Data/                           # Dataset folder (Ignored by Git)
‚îÇ   ‚îú‚îÄ‚îÄ README.md                   # Dataset documentation
‚îÇ   ‚îú‚îÄ‚îÄ test.csv
‚îÇ   ‚îú‚îÄ‚îÄ test_labels.csv
‚îÇ   ‚îî‚îÄ‚îÄ train.csv
‚îÇ
‚îú‚îÄ‚îÄ models/                         # Model folder (some files was ignored by Git)
‚îÇ   ‚îú‚îÄ‚îÄ model_card.md               # Model documentation
‚îÇ   ‚îî‚îÄ‚îÄ model.safetensors           # Fake model file
‚îÇ                                   
‚îú‚îÄ‚îÄ requirements.txt                # Global project dependencies
‚îî‚îÄ‚îÄ README.md                       # Project documentation
```

## ‚öôÔ∏è Installation & Setup

### 1. Set Up Python Environment
```bash
    git clone https://gitlab.esiea.fr/kamdemdjoko/nlp-final-project.git
    cd nlp-final-project
```

### 2. Set Up Python Environment
It is recommended to use a virtual environment.

```bash
    # Create virtual env
    python -m venv venv

    # Activate (Windows)
    .\venv\Scripts\activate
    # Activate (Mac/Linux)
    source venv/bin/activate
    
    # Install dependencies
    pip install -r api/requirements.txt
```

### 3. ‚ö†Ô∏è Download Model Weights
- Due to file size limits, the trained model weights (pytorch_model.bin) are not hosted on GitHub.
    - Option A (Pre-trained): Download the model folder from https://drive.google.com/drive/folders/1PEyyBh02zZG-b8hYsWFuPYVq_1szZA36?usp=sharing. 
    - Option B (Train Yourself): Run the 4_RoBERTa_Training.ipynb notebook to generate the model.

- Place the files: Unzip the content into the ./model/ folder.

- Your structure should look like: ./model/config.json, ./model/model.safetensors, etc.

### üöÄ Usage: Running the API
We use FastAPI to serve the model. The model is loaded into memory once at startup to ensure low latency.

*Start the Server*

Navigate to the api folder and run Uvicorn:

```bash
    cd api
    uvicorn app:app --reload
```
- The API will start at http://127.0.0.1:8000.

*Test the API*

- Option 1: Swagger UI (Browser)
    - Go to: http://127.0.0.1:8000/docs
    - Click POST /predict -> Try it out. 
    - Enter JSON: {"text": "You are amazing!"} or {"text": "You are an idiot."} or any other JSON
    - Click Execute.

- Option 2: cURL (Terminal)
```bash
    curl -X 'POST' \
  '[http://127.0.0.1:8000/predict](http://127.0.0.1:8000/predict)' \
  -H 'Content-Type: application/json' \
  -d '{"text": "This is a toxic comment test."}'
```

### üß† Model & Training Details

*Dataset*
- Source: Jigsaw Toxic Comment Classification Challenge.

- Size: ~160k training samples.

- Labels: toxic, severe_toxic, obscene, threat, insult, identity_hate.

- Handling: We treat this as a Binary Classification task (Toxic vs. Non-Toxic).

*Training Configuration*
- Base Model: roberta-base

- Batch Size: 16

- Learning Rate: 2e-5

- Epochs: 2

- Loss Function: CrossEntropyLoss

- Hardware: Trained on NVIDIA T4 GPU (Google Colab).

*Evaluation Metrics (Test Set)*

| Metric | Score (Test Set) |
| :--- |:-----------------|
| **ROC-AUC** | 0.97             |
| **F1-Score** | 0.67             |
| **Avg Latency** | ¬±20 ms           |

### üìä Latency Comparison
One of the main goals was to prove RoBERTa's efficiency over LLMs.

| Model           | Avg Latency GPU | Avg Latency CPU |
|:----------------|:----------------|----------------|
| **Miatral 7B**  | ¬±400 ms         | ¬±2-5s          |
| **RoBERTa**     | ¬±20 ms          | ¬±200ms         |

Note: API latency is measured using server-side headers (X-Process-Time) to exclude network overhead.

### üîç Explainability
We use SHAP (SHapley Additive exPlanations) to ensure the model isn't just guessing.

- Positive Contributors (Red): Words like "idiot", "hate", "stupid" push the score toward Toxic.

- Negative Contributors (Blue): Words like "thanks", "agree", "support" push the score toward Non-Toxic.

(See ./notebooks/05_explainability_shap.ipynb for visualizations)

### üìù License
This project uses the Jigsaw dataset (CC0) and the RoBERTa model (MIT). Project created by:
- Hind KHAYATI 
- Pape Mamadou DIAGNE
- Sarra HERELLI
- Sagnol Boutal KAMDEM DJOKO